<div align="center">
<!-- <img src = "https://github.com/GoThereGit/Chinese-AMR/blob/main/img/camrp2024.png" width=200> -->
</div>

# <p align="center"><font size=50><strong>第一届中文叙实性推理评测任务</strong></font></p> 
<p align="center"><font size=50><strong>Factivity Inference Evaluation 2025</strong></font></p> 

# 组织者和联系人

评测组织者：袁毓林（澳门大学教授，yulinyuan@um.edu.mo）、李斌（南京师范大学教授）

任务负责人&联系人：丛冠良（澳门大学博士生，guanliang.cong@connect.um.edu.mo）

团队成员：吴俊潮（澳门大学博士生）、汪月童（澳门大学本科生）、陈可盈（澳门大学硕士生）、寻天琦（澳门大学博士生）、陈阳（澳门大学博士生）、杨梓泓（澳门大学硕士生）


# 1 任务简介

叙实性推理（Factivity Inference, FI）是一种与事件真实性判断有关的语义理解任务，主要涉及语言使用中事实性信息的表达。在人类的会话交际中，叙实性推理能力表现为语言使用者可以从某些动词性语言成分（如“相信”“谎称”“意识到”等）的使用推知其他语言成分所描述的相关事件的真实性（真还是假）。 例如，从肯定句“他们意识到局面已经不可挽回”和相应的否定句“他们没有意识到局面已经不可挽回”上，都可以推理出存在这样一个事实：“局面已经不可挽回”。进行叙实性推理所使用的知识是一种受世界知识（world knowledge）影响较小、主要涉及语言内部各成分之间语义关系的分析性语言知识（analytical knowledge of language）。比如，上面例句中的动词“意识到”要求（预设）它的宾语“局面已经不可挽回”的所指为真，不管该动词前面有没有否定性词语。

叙实性推理和反事实推理（Counter-Factual Inference, CFI）是语义理解中与事件真实性判断有关的两种推理形式，可统称为“真实性推理”（Factuality Inference, FactI）。 相较而言，叙实性推理主要依靠谓词（predicates, 如动词）来表达。例如，从“约翰不知道罗昆是中国人”中“知道”这个动词的使用，可以推理出这样一个事实：“罗昆是中国人”。而反事实推理则主要依靠反事实条件句（counter-factual conditionals）来表达。例如，从“要不是消防队来得及时，大火就要烧到顶楼了”这个反事实条件句中，可以推理出两个事实：“消防队确实来得很及时”和“大火确实没有烧到顶楼”。

作为语言推理的重要导航机制和手段 ，真实性推理 具有明确的语言形式方面的线索，是机器进行文本蕴涵识别（textual entailment recognizing）、幻觉处理（hallucination solving）、信念修正（belief revision）等任务的重要的语义基础和形式依据，同时对信息检索、信息抽取、问题回答、情感分析等下游任务都具有重要价值。目前，大型语言模型（Large Language Models, LLMs）日益具备类人的与外界自主交互的能力，也被称为“智能体”（agent）。从话语中获取事实性信息及说话人对事件真实性判断的主观态度，这对于智能体的自主推理和人机交互而言是极为关键的。

为了提升大型语言模型对中文的语义理解能力，进一步实现机器对人类交际话语的深度理解，我们推出了第一届中文叙实性推理评测（Factivity Inference Evaluation, 简称FIE2025） 。

本次评测任务主要关注两方面的问题：
- LLMs的中文叙实性推理表现如何？不同LLMs在不同语境条件下的表现有何差异？
- 不同的提示词（prompts）编写方式（包括但不限于更改shots数量、使用CoT、更改提问句式等）对LLMs的叙实性推理的结果会产生何种影响？什么样的提示词设计可以最大程度上优化LLMs的中文叙实性推理表现？
参赛队伍需要根据所提供的测试集自行设计提示词，自主选择合适的模型参加测试，通过API方式向模型提问并获取回答。此次评测在大模型的选择、提示词的设计方式与具体的提问方式上均不设限制，鼓励尝试进行多样化、复合化测试手段以获得更好的回答表现。

# 2 评测数据 

## 2.1 数据规模与来源

本次评测以json格式提供评测集和样例集。由于评测对象为大型语言模型，故而不提供训练集和验证集。

评测集共包含6300条数据，分为人造语料集与真实语料集两个子集。其中，人造语料集842条，真实语料集5458条。

样例集共包含70条数据，分为人造语料集与真实语料集两个子集。其中，人造语料集20条，真实语料集50条。

人造语料由评测组织方团队人工构建与标注，并经过3位在叙实性方向颇有建树的语言学家核查校对；真实语料全部筛选自北京大学CCL语料库，并由评测组织方团队进行手工标注与校对。

## 2.2 数据字段

（1）	d_id：数据编号。编号采用“语料类型-数据号”的策略，其中“Art”表示“Artifactual”，“Nat”表示“Natural”，“ArtS”表示“Artifactual Sample”，“NatS”表示“Natural Sample”。

（2）	type：谓词的叙实性类型。此字段只出现在人造语料中。

（3）	predicate：谓词。谓词中大部分为动词，少部分为形容词。人造语料集涉及77个谓词，真实语料集涉及71个谓词。

（4）	text：背景句（主蕴含句）。此字段提供推理所需的语境，模型需要以此为依据来判断结论句的真值情况。

（5）	hypothesis：结论句（被蕴含句）。此字段提供推理所需的鉴别式，模型需要以背景句的内容来判断此句的真值情况。

（6）	option：题目选项。此字段反映模型可能的回答情况，包含4个键，“T”表示根据背景句推理出的结论句的真值为“真”，“F”表示根据背景句推理出的结论句的真值为“假”，“U”表示根据背景句推理出的结论句的真值为“不能确定”，“R”表示模型拒绝回答。

（7）	answer：模型回答。此字段反映模型实际的回答情况，包含4个值“T/F/U/R”，对应"option"字段的4个键。

## 2.3 数据样例

### 2.3.1 人造语料测试集数据样例：

```json
{
        "d_id": "Art_0272",
        "type": "反叙实",
        "predicate": "谎称",
        "text": "小张谎称会帮助小李。",
        "hypothesis": "小张会帮助小李。",
        "option": {
            "T": "真",
            "F": "假",
            "U": "不能确定",
            "R": "模型拒绝回答"
        }
    }
```

### 2.3.2 真实语料测试集数据样例：

```json
{
        "d_id": "Nat_5507",
        "predicate": "记得",
        "text": "侯博还记得她叫白冰，是那个大街上对他吐露过内心恋情的女生。",
        "hypothesis": "她确实叫白冰。",
        "option": {
            "T": "真",
            "F": "假",
            "U": "不能确定",
            "R": "模型拒绝回答"
        }
    }
```

### 2.3.3 人造语料样例集数据样例：

```json
{
        "d_id": "ArtS_001",
        "type": "非叙实",
        "predicate": "估计",
        "text": "小张估计小李没生病。",
        "hypothesis": "小李生病了。",
        "option": {
            "T": "真",
            "F": "假",
            "U": "不能确定",
            "R": "模型拒绝回答"
        },
        "answer": "U"
    }
```

2.3.4 真实语料样例集数据样例：

```json
{
        "d_id": "NatS_001",
        "predicate": "抱怨",
        "text": "世纪交替之际，联合国各会员国接受会费分摊比额新方案，体现了各国决心加强联合国作用的政治意愿。美国曾长期抱怨自己分摊的比例过大，要求削减会费。现在，美国的这一要求已在很大程度上得到满足。",
        "hypothesis": "美国分摊的比例确实过大。",
        "option": {
            "T": "真",
            "F": "假",
            "U": "不能确定",
            "R": "模型拒绝回答"
        },
        "answer": "T"
    }
```

## 2.4	任务描述

- 参赛队伍需要将测试集中的数据以API访问的方式逐条发送给被测试的大模型，要求大模型以"text"字段值为依据来判断"hypothesis"字段值的真值情况，记录模型的返回结果，并将结果整理为输出文件。
- 真值情况包括三种：
  - 若根据"text"判定"hypothesis"为真，意味着根据背景句推出结论句的真值为“真”，此时请输出“T”；
  - 若根据"text"判定"hypothesis"为假，意味着根据背景句推出结论句的真值为“假”，此时请输出“F”；
  - 若根据"text"不能判定"hypothesis"的真假，意味着根据背景句推出结论句的真值为“不能确定”，此时请输出“U”。
- 若模型拒绝回答，请输出“R”。
- 若遇到其他问题，请邮件联系任务负责人。

## 2.5	数据使用说明与提示

- 参赛队伍需要参考数据内容自行设计与大模型对话时的提示词，因此在数据中未设置"question"字段。
- 提示词格式不限，但在提示词中必须同时包含当前数据中"text"和"hypothesis"字段的内容。
- "type"和"predicate"字段在提示词中既可以出现，也可以不出现。请自行决定是否使用、如何使用"type"和"predicate"字段。
- 提示词的设计可以进行多样化尝试，如提供更多数量的shots、要求使用CoT、要求进行一致性投票、告知动词类型、告知动词的叙实性类型、变换提问句式等等。

## 2.6	输出要求

- 输出文件须为json格式，其中每条数据只需包含"d_id"和"answer"两个字段即可。
- 由于所有题目都是单选题，一条数据的"answer"处只允许填写一个值。
- 开放测试中，参赛队伍使用的所有资源需要在最终提交的技术报告中给予详细说明。但无论哪种模态，均不可使用人工修正自动解析结果的方式。

## 2.7	输出样例

```json
{
        "d_id": "Nat_0001",
        "answer": "T"
    }
```

# 3	模态设置 

本次评测只包含开放测试（Open Modality）。参赛队伍可自由选择参与测试的大型预训练语言模型，但不允许使用外部资源。

实验中的所有代码与结果请妥善保存，以备查用。禁止人工修正回答。

# 4 评价标准 

本次评测采用总正确率作为评价指标:

<div align=center>
<img src="metric.jpg" width="400px">
</div>

其中，**total\_acc** 为总正确率，**correct\_art** 为人造语料集中模型推理正确的数据量，**correct\_nat** 为自然语料集中模型推理正确的数据量，**total\_art** 为人造语料集中的数据总量，**total\_nat** 为自然语料集中的数据总量。


# 5	技术报告要求 

参与评测必须提交技术报告，不提交技术报告的队伍成绩将不会获得认可。报告要求如下：

1.	报告可由中文或英文撰写。
2.	报告统一使用CCL 2025的论文模板。
3.	报告正文不得超过4页，参考文献页数不限。
4.	报告应至少包含以下四个部分：模型介绍、评测结果、结果分析与讨论和参考文献。
   
如打算向会议投稿，请参考下面的论文格式：

会议投稿需统一使用LaTeX模板。提交的论文最多包含 6 页正文，参考文献页数不限。由于本次会议采用双盲审稿，作者姓名和单位不能出现在投稿的论文中。因此，作者的自引不可采用“我们提出”的方式，而是用“作者名字提出…”。不符合这些要求的论文将不经过完整的审稿流程而直接被拒稿。论文模板下载链接：http://cips-cl.org/static/CCL2024/downloads/ccl2023_template.zip。

# 6 评测赛程 
- 2025年1月：评测任务发布、参赛队伍报名；
- 2025年1月上旬：组织方发布正式测试集；
- 2025年1月- 4月：各参赛队伍开展评测；
- 2025年4月31日：参赛队伍提交中文叙实性推理评测任务技术报告（中文或英文），用于审稿；
- 2025年5月：组织方对技术报告开展集中评审，提出修改意见；各参赛队伍修改技术报告。
- 2025年6月1日：组织方公布中文叙实性推理评测结果；
- 2025年6月10日：向评测研讨会提交技术报告最终版；
- 2025年6月20日：评测论文审稿 & 录用通知；
- 2025年6月25日：评测论文Camera-ready版提交；
- 2025年7月1日：评测论文纠错排版 & 提交ACL/CCL Anthology收录；
- 2025年8月：CCL 2025技术评测研讨会。

# 7 任务奖项 

本届评测将设置一、二、三等奖，奖项按总正确率从高到低颁发。其中，一等奖0-1名，二等奖0-2名，三等奖0-3名。

# 8 报名方式 

请仔细阅读《第一届中文叙实性推理评测FIE2025参赛协议》和《第一届中文叙实性推理评测FIE2025数据集使用许可》，然后点击报名链接进行报名。 

# 9 任务网址 

https://github.com/UM-FAH-Yuan/Chinese-FIAC
