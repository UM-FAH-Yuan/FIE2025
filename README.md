<div align="center">
<!-- <img src = "https://github.com/GoThereGit/Chinese-AMR/blob/main/img/camrp2024.png" width=200> -->
</div>

<p align="right"><font size=50><strong>[English Version[(https://github.com/UM-FAH-Yuan/FIE2025/blob/main/README_EN.md)</strong></font></p> 

# <p align="center"><font size=50><strong>第一届中文叙实性推理评测任务</strong></font></p> 
<p align="center"><font size=50><strong>Factivity Inference Evaluation 2025</strong></font></p> 

# 近期更新 

![Static Badge](https://img.shields.io/badge/20250503-Q%20%26%20A-blue?style=plastic)

- Q1: 我刚刚测试了这个天池平台的的提交方式，可能格式不对，所以一直提交失败。建议主办方考虑提供在正式测评集后面提供一个答题模板，就是对应您的测评集。像我刚刚试了，还是没办法提交，可能顺序不对还是其他原因。 

```json
{
"d_id": "nat_acc_001",
"answer": "U"
}
```

请问是否可以向我们提供一个递交答案的模板？这样可以规避掉此类问题，模板顺序与之测评集顺序相同，最起码要告知我们先人工还是先自然的顺序。
- A1: 感谢您的反馈，我们已收到！我们将首先与天池平台沟通，~~并尽快更新 GitHub 上的提交代码示例。~~ ![Static Badge](https://img.shields.io/badge/20250504__updated-blue?style=plastic)
提交代码的示例已在本站更新（[点此跳转](#daimashili)）。需要注意的是，天池平台上结果页面显示的“art_acc”与“nat_acc”是指此结果文件中人造语料与自然语料的正确率，与数据id无关。
- Q2: 我们还是对ArtS_085与ArtS_087的数据标注有疑问。
- A2: 关于您提到的“披露”一词的标注争议，我们目前仍倾向于维持现有的标注结果。为确保数据的稳定性，我们决定在比赛正式开始前不再调整样例集与评测集的相关内容。不过，关于该类争议，我们非常欢迎在评测结束后与大家进一步探讨。这类动词确实非常值得深入研究，我们认为贵团队的理解与现有标注之间的差异，主要可能在于对确定性程度的判断以及不同选项外延范围的把握。
  考虑到本次任务初期设定的标注模式为“真 / 假 / 不能确定”的三分类机制，确实可能会导致个别数据存在争议。然而，我们认为这并不妨碍各参赛队伍对任务本身的有效评估。正如我们在任务发布时所强调的，本次评测的主要目标，是希望推动计算语言学和自然语言处理领域对中文文本推理，特别是叙实性推理问题的关注。我们由衷感谢您和团队在这一方向上的深入思考与宝贵反馈。在未来的第二届中文叙实性推理评测中，我们将充分吸纳各方意见，进一步优化标注模式，以期更精准地衡量模型在叙实性与事实性推理方面的表现。
- Q3: 请问一下现在提交参赛协议还可以吗？
- A3: 可以的。在比赛排名公布之前提交参赛协议都可以。我们建议参赛队伍尽早提交签署好的参赛协议，以防在统计排名时因队伍信息不全而被取消成绩。

![Static Badge](https://img.shields.io/badge/20250501-Q%20%26%20A-blue?style=plastic)

- Q1: 能否两个模型配套使用？这属于微调模型赛道还是不微调模型赛道？
- A1: 如果所谓“配套使用”的意思是用多种模型分别开展测试，然后将最好的结果组装起来，那样是不行的，属于作弊行为。首先，我们组织此次评测任务的主要目的之一就是为了调查目前各个语言大模型在中文叙实性推理上的情况如何，如果采用拼凑数据的办法提交结果就丧失了调查的意义；如果在评测中使用了多个模型，应当就每个模型分别给出完整的评测结果，否则无法比较不同模型表现之间的差距。其次，正如我们在任务说明会中说明的那样，组织方在遴选和推荐技术报告时并非以评测成绩作为唯一标准，队伍在技术和设计上的创新点也是重要的考虑因素。因此没有必要为了追求更高的正确率而拼凑数据。
- Q2: 有成员提出“披露”和“揭露”的测试问题，详见人造语料中“84-87；236-238”。
- A2: 经过讨论，我们认为这几条数据标注不需要修改。
- Q3: 对于非微调赛道，使用的模型的限制的疑问。是否允许LLM调用工具的模式（或RAG的模式）？对于使用的模型是否有一个规定界限？这个问题的核心在于是否需要额外规定一下什么情况下的模型是不可用的，以做到比赛尽量公平。
- A3: 我们认为叙实性不仅是一个纯粹的句法概念，而是一个涉及词汇、句法、语义、语用等多个语言学层面的复杂范畴。因而引入单一的外部工具可能不一定能明显提高推理的成功率。但我们鼓励大家探索引入其他外部资源来探索叙实性判断，例如可以引入其他外部资源的分析结果或其他数据集的数据作为提示词的一部分，等等。总体来说，模型问题需要参赛队伍在技术报告中说明自己的算法和资源，因此不必过于担心公平性问题。由于各种评测都存在作弊的可能性，所以需要大家提交技术报告及源代码，以尽可能保证公平。
- Q4: 对于非微调赛道，对“微调”的进一步疑问。使用training free的方案是否符合非微调赛道的要求？这类技术本质上也没有“更新参数”，而是进行了一些参数合并等操作。更进一步来说，微调是否意味着进行梯度下降操作，如果没有梯度下降的拟合就是非微调？
- A4: 对模型的解码策略进行修改，而没有修改模型权重的做法是可以的。但我们定义的非微调至少为“没有梯度下降的拟合且没有对模型权重进行修改”。

![Static Badge](https://img.shields.io/badge/20250430-Q%20%26%20A-blue?style=plastic)

- Q1: 签字后的协议扫描PDF应该发给谁，是否是通过邮件的方式发送，如何确保已经在参赛队伍中。签字人需要有老师签字吗？此外，指导老师是否需要也加入天池的参赛队伍中，还是得到排名后，在后续的步骤中填写？这个问题是基于，假设得到排名有奖状，奖状上的署名问题.
- A1: 《参赛协议》的最后预留了需要参赛队伍自行填写的信息，请联系人/队长填写好后将参赛协议保存为pdf格式，通过邮件发送回组织方联系人邮箱（guanliang.cong@connect.um.edu.mo）。（联系人/队长的主要职责是负责与组织方保持邮件沟通）我们在收到签署好的参赛协议后会反馈给队伍一个队伍编号，收到此编号可以确定报名成功。
由于参与评测需要提交技术报告，如果老师需要署名，可以在技术报告中体现（如果排名较高）。
~~证书内容目前尚未确定，但在制作证书前，我们会再次和获奖队伍确认文字信息，具体的署名顺序等问题可届时再最终确定。~~ ![Static Badge](https://img.shields.io/badge/20250501__updated-blue?style=plastic)经过与评测工作坊官方确认，作者的数量在提交技术报告时确认，之后不得更改。
- Q2: qq群在哪里可以找到加入？
- A2: 参赛队伍需要将签署好的《参赛协议》以附件形式发送至组织方联系人邮箱（guanliang.cong@connect.um.edu.mo），组织方在收到后会反馈一个队伍编号，同时发送腾讯QQ交流群的二维码，参赛队员可扫描二维码加入群聊。钉钉群的二维码已经在天池平台页面挂出（[点此跳转](https://tianchi.aliyun.com/competition/entrance/532342)）
- Q3: 对于非微调赛道，是否允许多智能体协同的模式？
- A3: 当然！我们鼓励参赛队伍采用多样化手段开展评测，因此可以使用多智能体协同。

![Static Badge](https://img.shields.io/badge/20250428-Q%20%26%20A-blue?style=plastic)

- Q1: 您能否提前告知我们，人造语料中哪些题目进行了修改？
- A1: 此次人造语料的修改之处已上传至本站，请见[ArtS-20250427updated.csv](https://github.com/UM-FAH-Yuan/FIE2025/blob/main/Sample_Set/ArtS-20250427updated.csv)。
- Q2: 针对主办方团队提出的“微调”与“不微调”赛道，提供略有差异的测评集。我们表示赞同，有个想法仅供主办方参考：之前邮件中所提及的“假装”类词汇不计入正确率，但我们认为可作为两种赛道的区别点。因为“微调”赛道所学习有争议的题目数据是“U”，但是“不微调”赛道的标注指令，受之前论文和自己标注指令的影响，输出的结果一定会有所差异。我们之前试过，才会提出对“假装”类题目某些答案的疑惑。故可以开赛前就说明微调赛道的就按数据集的数据来学习，不微调赛道对“假装”类要有自己的标注理解，最后的结果不算“假装”的正确率。如果有队伍交了相同答案的话，就会在“假装”上答案一致，我们就可以详细审核其代码，一方面更加公平，一方面可以推动“假装”类的研究，一举多得。
- A2: 关于您提到的结果文件的建议，我们认为简单根据题目数量区分开微调与不微调赛道更容易操作（![Static Badge](https://img.shields.io/badge/20250504__updated-blue?style=plastic)目前我们是通过评测集中区分微调赛道和不微调赛道的数据id来实现的。具体来说，微调赛道的数据以“_FT”结尾，不微调赛道的数据以“_prompt”结尾）。本次参赛队伍的背景复杂、研究方向不一，不一定都对“假装”这类语言学本体问题感兴趣，因此要求全部队伍都提出自己的见解可能有困难。
- Q3: 如何知道我们团队的操作究竟属于微调赛道还是不微调赛道？例如调节温度等专属于微调AI模型的相关数据是否属于微调模型？
- A3: 关于微调与不微调的区分已在昨晚的会议中说明，详见会议视频回放。但关于您在邮件中提到temperature选项的设定，我们认为这不属于微调模型的范围，那只是在调节模型回答的一致性，并不涉及模型本身的改动。
- Q4: 想询问您昨晚的任务说明会最后李老师提到的几篇优秀论文是否方便共享到QQ群（因为讨论区的内容回放无法看到）？
- A4: 昨晚提到的推荐论文信息如下：

中文论文推荐参考以下几个网站：

https://aclanthology.org/events/ccl-2024/#2024ccl-3

https://aclanthology.org/events/ccl-2023/#2023ccl-3

https://aclanthology.org/2023.ccl-3.12.pdf

https://aclanthology.org/2023.ccl-3.9.pdf

英文论文推荐参考CoNLL和SemEval的会议论文。

![Static Badge](https://img.shields.io/badge/20250427-UPDATE-brightgreen?style=plastic)

今晚我们召开了第二次任务说明会，会议录像链接请见下方：

录制：FIE2025任務說明會20250427

日期：2025-04-27 20:05:07

录制文件：[https://meeting.tencent.com/crm/2BLk3pQk47](https://meeting.tencent.com/crm/2BLk3pQk47)（无需密码）

[点此查看幻灯片](https://github.com/UM-FAH-Yuan/FIE2025/blob/main/meeting_record_20250427.pdf)

![Static Badge](https://img.shields.io/badge/20250426-Q%20%26%20A-blue?style=plastic)

- Q1: 如果在阿里云天池平台报名后，还需要在github内的链接进行报名吗？
- A1: 在阿里云天池平台报名后，还需要签署一个参赛协议。目前我们正在调整天池页面，将文件放上去。您可以先阅读这两份文件，然后将签署好的参赛协议通过此邮件反馈给我们。
- Q2: 是否阿里云天池平台上的排行榜届时只是一个参考？实际的评测排名是否应该在5月15日参赛队伍提交技术报告后，6月1日由组织方对结果进行公布，而非我们能见到实时结果？
- A2: 是的，天池排行榜仅作为实时排名参考。参赛队伍需要提交技术报告后才能确认成绩。天池排行榜的结果上传页面不会在~~15号~~12号马上关闭，但这次评测的成绩会在~~15号~~12号截止，以便后续安排后续参会论文提交等事宜。评测的结果大约会在评测截止后一周左右公布。

![Static Badge](https://img.shields.io/badge/20250425-Q%20%26%20A-blue?style=plastic)

- Q1: 按自设指令、语感和相关论文，团队成员对人造样例集67、80、89、90、225、82与233的答案提出质疑。今天团队成员在研究样例的时候，发现新修订的答案把381、382和665、666例做了修改，但对533、534没有修改，我们讨论后觉得这三组样例可以并在一起，视作为同一类，均为U。想知道主办方对533和534例的答案的看法如何？
- A1: 对于您提出的问题，我们团队进行了认真讨论，认为前次修订后的自然语料样例集答案无误，暂不修改；人工语料样例集的数据则进行了极少量调整，修订后的完整版本将于本周末上传。![Static Badge](https://img.shields.io/badge/20250504__updated-blue?style=plastic)此次人造语料的修改之处已上传至本站，请见[ArtS-20250427updated.csv](https://github.com/UM-FAH-Yuan/FIE2025/blob/main/Sample_Set/ArtS-20250427updated.csv)。
- Q2: 建议主办方第一届比赛只允许队伍参加一个赛道，不能两个赛道都参加。因为天池平台递交最终结果，完全可以把微调模型的赛道结果递交到不微调模型的里面。最后只看结果的话，微调模型的正确率肯定大于不微调模型。除非参加两个赛道的队伍，能拿出强有力证据来证明自己的结果，为保证第一届比赛的顺利举行，建议严格分开。
- A2:关于赛道设置，我们认为当前的安排较为合适。第一，本次评测任务的初衷是推动学界对中文叙实性推理问题的关注与研究。如果规定每支队伍只能参加一个赛道，虽然有助于减少作弊风险，但也可能在一定程度上限制队伍深入开展多角度研究的可能性。
第二，我们已在任务说明文档中明确要求所有参赛队伍保存代码以备复现，在评测结果核查阶段，我们将要求排名靠前的队伍提交完整代码。
第三，我们也会持续提醒各参赛队在提交结果时，务必选择对应赛道的正确结果文件。
第四，我们正在考虑对微调与非微调两个赛道提供略有差异的评测集（例如为微调赛道额外增加一道题），以降低提交错误的风险。
- Q3: 团队成员对测评时间只有5天，产生疑虑。我们团队成员建议主办方是5天给微调模型赛道，7-10天给不微调模型赛道的。因为不微调模型的手动标注指令时间远大于微调模型不手动标注指令的时间。
- A3: 关于提交时间的安排，我们在综合多方因素后，决定参照国际惯例，将两个赛道的提交时间由原定的5天统一延长至7天，并略微提前评测集的开放时间。具体安排将在4月27日的任务说明会上详细说明。（![Static Badge](https://img.shields.io/badge/20250504__updated-blue?style=plastic)最新时间安排请见本站#评测赛程）

![Static Badge](https://img.shields.io/badge/20250423-Q%20%26%20A-blue?style=plastic)

- Q1: 评测论文的提交方式是否与技术报告相同，还是需单独通过 OpenReview 提交？
- A1: 是的。组织方会在5月20日至6月1日期间审核技术报告，并推荐成绩较好的队伍将技术报告扩写成论文，向工作坊投稿。无论是技术报告还是评测论文，投稿的通道连接均为[OpenReview](https://openreview.net/group?id=cips-cl.org/CCL/2025/Workshop/Eval)。注意不要点进主会议链接。
- Q2: 技术报告截止为5月15日（暂定），而 OpenReview 系统显示论文截止为5月11日。请问我们应以哪一个时间为准？
- A2: 目前OpenReview系统显示已更新，技术报告的提交日期为：Submission Start: May 20 2025 12:00AM UTC-0, Submission Deadline: Jun 01 2025 12:00AM UTC-0。提交时间以此为准。

![Static Badge](https://img.shields.io/badge/20250421-Q%20%26%20A-blue?style=plastic)

- Q1: 是否允许大模型在生成答案时回复除T/F/U/R的其他内容？例如在产生cot思维链后再人工将答案再提取出来。
- A1: 可以的。只要模型的回答中有关答案的部分是明确的（即明确回答了“真/假/不能确定”或“正确/错误/不能确定”等内容）、前后一致的（即单次回答中没有出现前后矛盾的答案）即可。如果人工提取答案，需要在技术报告中给出清晰、一致、可复现的提取规则，并保存好代码以备抽查。
- Q2: 是否允许使用多轮对话？
- A2: 理论上我们允许采取包括多轮对话在内的各种方式进行提问，但~~不建议~~禁止在对话中通过人工诱导的方式纠正模型回答，例如“你说错了，我认为答案应该是…”，这就很难测试出模型对叙实性推理的真实水平。

![Static Badge](https://img.shields.io/badge/20250417-Q%20%26%20A-blue?style=plastic)

- Q1: 团队成员是否需要注册天池平台的信息，以参加后续测评活动。按平台规则所写，不及时注册会影响后续测评结果，以及测评结果递交是否也在天池平台进行？
- A1: 我们的任务将会依托依托天池的网站平台开展，因此需要各位队伍自行注册天池平台的账号，并点击[任务四链接](https://tianchi.aliyun.com/competition/entrance/532342?spm=a2c22.12281949.0.0.7e923b74oFk7BK)报名参赛。
届时评测集数据将在天池平台上提供~~调用方式~~ （![Static Badge](https://img.shields.io/badge/20250504__updated-blue?style=plastic)5月6日-12日参赛队伍可直接在天池平台上下载评测集数据。务请看清名称下载，不要使用错误赛道的评测集文件），同时各队伍也需要将自己的评测结果文件上传至结果提交页面。组织方计划在开赛前一周左右再组织一次任务说明会，与各个队伍再次确认评测内容、评测方法、参赛方式、结果评价方式、技术报告和论文要求等细节。说明会的具体日期我们将通过邮件通知。
- Q2: 样例集中出现的动词是否会在测评集中重复出现，比如：“假装”在样例集中出现，是否会在测评集中继续出现？进一步来说，样例集中的动词是否覆盖测评集中的动词？
- A2: 样例集中出现的动词会在测评集中重复出现，但只涵盖测评集包含动词数量的一半。为了测试模型的泛化能力，我们刻意控制了出现在样例集中的动词数量，因此评测集中会出现更多新的动词。
- Q3: 关于“假装”一词，有团队成员想知道原来的样例集（文件名：natural_sample_202412311817.json）中的25、26是否答案有误。因为我们是按照这个答案并结合相关论文得出指令，来指导新自然语料样例集中的“假装”类，尤其是265-272；277-278。但根据主办方您之前最新更改的答案对此进行修正，我们正在重新研究相关指令搭建。就有团队成员想起了旧样例集中的题目，想知道之前的样例中的答案是否正确？
- A3: 关于“假装”的题目，我们经过多番商讨，最终决定：所有有关“假装”的题目在评测集中保留，但不用于计算正确率。
我们的这样做的理据如下：“假装”在叙实性研究中确实是一个较为特殊的动词，它与生物的“伪装”行为有关。就人类而言，假装的对象既可以是一种动作也可以是一种状态。当句子主语假装“处于某种状态”时，其补足语小句的所含命题可以被推断为假，研究者们基本对这一点没有异议；麻烦的是，当句子主语假装做出某种动作时，学界在“补足语小句所描述的动作是否完成”这个问题上存在观点分歧。究其原因，这是由于人类“假装”行为的实施包含“产生伪装意图”和“做出某个动作/表现出处于某种状态”两个部分。当人们假装“做出某个动作”时，是用“假意图驱动真动作”。我们认为，“假装”行为的实施仍然意味着句子主语做出了某种伪装动作，尽管学者们对动作的完成质量和实现程度可能有不同看法。因此，为了避免由于学者观点差异而造成的答案分歧，我们决定在评测集中保留所有有关“假装”的题目（以便学界后续就该问题开展深入研究），但无论模型如何作答最终都不计入成绩。同时为了尽量保持评测集数据的数据量不变，我们将从其他动词中再选出几条数据用于填补“假装”题目的空缺，这将在最终测评集发布之前完成，对各队伍开展测试基本无影响。

![Static Badge](https://img.shields.io/badge/20250408-Q%20%26%20A-blue?style=plastic)

- Q1: 关于组织方介绍的微调模型的样例代码，我运行了步骤1到4，看起来'data/factivity_test_data.json`文件是空的。这是正常的么？或者我们需要把以前的Arts_20250325.json和NatS_20250407.json当做data/factivity_test_data.json进行试验？
- A1: test_data的文件需要您自己来设置。我们之前做基准测试的时候使用的是正式测试集中的一部分。您可以尝试把我们这次提供的带答案的样例集分割成两部分，其中一部分用于微调训练，另一部分用于验证模型效果。
- Q2: 为什么我的微调模型达不到基线水平？
- A2: 您提到无法复现我们基线结果的问题在目前这个阶段来看可能是正常的：由于我们没有额外提供验证集，您在训练模型时势必需要从样例集中拆分出一部分数据用作验证，这样模型的训练效果就很可能会因此受到影响。我们预计，在正式开展测试时，当您使用完整的样例集进行微调训练时，模型的表现应当会得到改善。

![Static Badge](https://img.shields.io/badge/20250328-UPDATE-brightgreen?style=plastic)

1. 为了方便有需求的队伍进行微调，我们录制了一个访谈式的视频，以简要介绍一种利用Llama Factory来进行lora微调的方法。如需参考，请见https://meeting.tencent.com/crm/KPGoe6dYa8；密码：0328。
2. 我们在Github上传了基准测试使用的微调代码。如需参考，请见本站2.11的内容（[点此跳转](#weitiao)）。

# 报名方式 

请仔细阅读[《第一届中文叙实性推理评测FIE2025参赛协议》](https://github.com/UM-FAH-Yuan/FIE2025/blob/main/Agreement%20%26%20License/%E7%AC%AC%E4%B8%80%E5%B1%8A%E4%B8%AD%E6%96%87%E5%8F%99%E5%AE%9E%E6%80%A7%E6%8E%A8%E7%90%86%E8%AF%84%E6%B5%8B%EF%BC%88FIE2025%EF%BC%89%E5%8F%82%E8%B5%9B%E5%8D%8F%E8%AE%AEv3-20250123.pdf)和[《第一届中文叙实性推理评测FIE2025数据集使用许可》](https://github.com/UM-FAH-Yuan/FIE2025/blob/main/Agreement%20%26%20License/%E7%AC%AC%E4%B8%80%E5%B1%8A%E4%B8%AD%E6%96%87%E5%8F%99%E5%AE%9E%E6%80%A7%E6%8E%A8%E7%90%86%E8%AF%84%E6%B5%8BFIE2025%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BD%BF%E7%94%A8%E8%AE%B8%E5%8F%AFv3-20250123.pdf)，然后点击报名链接进行报名。 （如出现PDF无法显示的情况，请更换浏览器，建议使用Chrome浏览器）

报名链接：https://tianchi.aliyun.com/competition/entrance/532342/introduction

报名注意事项：

- 队长不能作为队员参与其他队伍；
- 队伍成员人数不限。
- 本任务依托阿里云天池平台开展评测，队伍在前文链接报名后请自行前往阿里云天池平台注册账号并点击报名按钮参赛。

# 评测赛程 
- 2025年1月：评测任务发布、参赛队伍报名；
- 2025年2月下旬：组织方召开任务说明会；
- 2025年3月1日：CCL官方正式发布所有评测任务；
- 2025年3月-5月：各参赛队伍进行模型微调及提示词设计（组织方可为各参赛队伍提供技术指导，指导方式与时间待定）；
- 2025年4月27日：组织方召开第二次任务说明会；
- 2025年5月3日：开放天池平台提交结果页面（用以测试提交程序）；
- 2025年~~4月中下旬~~ ~~5月10日~~5月6日-12日：公布评测集；
- 2025年~~4月30日~~ ~~5月15日~~5月20日：参赛队伍提交中文叙实性推理评测任务技术报告（中文或英文），用于审稿；
- 2025年5月：组织方对技术报告开展集中评审，提出修改意见；各参赛队伍修改技术报告。
- 2025年6月1日：组织方公布中文叙实性推理评测结果；
- 2025年6月10日：向评测研讨会提交技术报告最终版；
- 2025年6月20日：评测论文审稿 & 录用通知；
- 2025年6月25日：评测论文Camera-ready版提交；
- 2025年7月1日：评测论文纠错排版 & 提交ACL/CCL Anthology收录；
- 2025年8月：CCL 2025技术评测研讨会。

备注：
- 以上时间为北京时间。
- 本任务在4月30日前均接受新的队伍报名参赛，但后期报名的队伍必须以相同标准、按时提交技术报告。

# 1 任务简介

叙实性推理（Factivity Inference, FI）是一种与事件真实性判断有关的语义理解任务，主要涉及语言使用中事实性信息的表达。在人类的会话交际中，叙实性推理能力表现为语言使用者可以从某些动词性语言成分（如“相信”“谎称”“意识到”等）的使用推知其他语言成分所描述的相关事件的真实性（真还是假）。 例如，从肯定句“他们<strong>意识到</strong><ins>局面已经不可挽回</ins>”和相应的否定句“他们<strong>没有意识到</strong><ins>局面已经不可挽回</ins>”上，都可以推理出存在这样一个事实：“局面已经不可挽回”。进行叙实性推理所使用的知识是一种受世界知识（world knowledge）影响较小、主要涉及语言内部各成分之间语义关系的分析性语言知识（analytical knowledge of language）。比如，上面例句中的动词“意识到”要求（预设）它的宾语“局面已经不可挽回”的所指为真，不管该动词前面有没有否定性词语。

叙实性推理和反事实推理（Counter-Factual Inference, CFI）是语义理解中与事件真实性判断有关的两种推理形式，可统称为“真实性推理”（Factuality Inference, FactI）。 相较而言，叙实性推理主要依靠谓词（predicates, 如动词）来表达。例如，从“约翰不知道罗昆是中国人”中“知道”这个动词的使用，可以推理出这样一个事实：“罗昆是中国人”。而反事实推理则主要依靠反事实条件句（counter-factual conditionals）来表达。例如，从“要不是消防队来得及时，大火就要烧到顶楼了”这个反事实条件句中，可以推理出两个事实：“消防队确实来得很及时”和“大火确实没有烧到顶楼”。

作为语言推理的一种重要的导航机制和手段，真实性推理具有明确的语言形式方面的线索，是机器进行文本蕴涵识别（textual entailment recognizing）、幻觉处理（hallucination solving）、信念修正（belief revision）等任务的重要的语义基础和形式依据，同时对信息检索、信息抽取、问题回答、情感分析等下游任务都具有重要的价值。目前，大型语言模型（Large Language Models, LLMs）日益具备类人的与外界自主交互的能力，也被称为“智能体”（agent）。从话语中获取事实性信息及说话人对事件真实性判断的主观态度，这对于智能体的自主推理和人机交互的顺畅性而言是极为关键的。

为了提升大型语言模型对中文的语义理解能力，进一步实现机器对人类实时交际话语的深度理解，我们推出了第一届中文叙实性推理评测（Factivity Inference Evaluation 2025, 简称FIE2025）。

本次评测任务主要关注两方面的问题：
- LLMs的中文叙实性推理表现如何？不同LLMs在不同语境条件下的表现有何差异？
- 不同的提示词（prompts）编写方式（包括但不限于更改shots数量、使用CoT、更改提问句式等）对LLMs的叙实性推理的结果会产生何种影响？什么样的提示词设计可以最大程度上优化LLMs的中文叙实性推理表现？
参赛队伍需要根据评测组织方所提供的测试集自行设计提示词，自主选择合适的模型参加测试，通过API方式向模型提问并获取回答。此次评测在语言大模型的选择、提示词的设计方式与具体的提问方式上均不设限制，鼓励尝试进行多样化、复合化的测试手段，以获得更好的回答表现。

# 2 评测数据 

## 2.1 数据规模与来源

本次评测以JSON格式提供样例集和评测集。由于评测对象为大型语言模型，故而不提供训练集和验证集，参赛队伍可将样例集数据可用于模型微调训练，并自行从中划分出验证集。

评测集共包含约2000条数据，分为人造语料集与真实语料集两个子集。其中，人造语料集约500条，真实语料集约1500条。

样例集共包含1000条数据，分为人造语料集与真实语料集两个子集。其中，人造语料集300条，真实语料集700条。

人造语料由评测组织方团队人工构建与标注，并经过3位在叙实性方向颇有建树的语言学家核查校对；真实语料全部筛选自北京大学CCL语料库，并由评测组织方团队进行手工标注与校对。

## 2.2 数据字段

（1）	d_id：数据编号。编号采用“语料类型-数据号”的策略，其中“Art”表示“Artifactual”，“Nat”表示“Natural”，“ArtS”表示“Artifactual Sample”，“NatS”表示“Natural Sample”。

（2）	type：谓词的叙实性类型。此字段只出现在人造语料中。

（3）	predicate：谓词。谓词中大部分为动词，少部分为形容词。人造语料集涉及77个谓词，真实语料集涉及71个谓词。

（4）	text：背景句（主蕴含句）。此字段提供推理所需的语境，模型需要以此为依据来判断结论句的真值情况。

（5）	hypothesis：结论句（被蕴含句）。此字段提供推理所需的鉴别式，模型需要以背景句的内容来判断此句的真值情况。

（6）	option：题目选项。此字段反映模型可能的回答情况，包含4个键，“T”表示根据背景句推理出的结论句的真值为“真”，“F”表示根据背景句推理出的结论句的真值为“假”，“U”表示根据背景句推理出的结论句的真值为“不能确定”，“R”表示模型拒绝回答。

（7）	answer：模型回答。此字段反映模型实际的回答情况，包含4个值“T/F/U/R”，对应"option"字段的4个键。在我们设定的正确回答中实际上没有“R”，“R”键的设立主要是为了在评测过程中提醒参赛队伍修改提示词以正确获得模型回答。

## 2.3 数据样例

### 2.3.1 人造语料测试集数据样例：

- 不微调赛道

```json
{
        "d_id": "Art_0272_prompt",
        "type": "反叙实",
        "predicate": "谎称",
        "text": "小张谎称会帮助小李。",
        "hypothesis": "小张会帮助小李。",
        "option": {
            "T": "真",
            "F": "假",
            "U": "不能确定",
            "R": "模型拒绝回答"
        }
    }
```

- 微调赛道

```json
{
        "d_id": "Art_0272_FT",
        "type": "反叙实",
        "predicate": "谎称",
        "text": "小张谎称会帮助小李。",
        "hypothesis": "小张会帮助小李。",
        "option": {
            "T": "真",
            "F": "假",
            "U": "不能确定",
            "R": "模型拒绝回答"
        }
    }
```

### 2.3.2 真实语料测试集数据样例：

- 不微调赛道

```json
{
        "d_id": "NatT_5507_prompt",
        "predicate": "记得",
        "text": "侯博还记得她叫白冰，是那个大街上对他吐露过内心恋情的女生。",
        "hypothesis": "她确实叫白冰。",
        "option": {
            "T": "真",
            "F": "假",
            "U": "不能确定",
            "R": "模型拒绝回答"
        }
    }
```

- 微调赛道

```json
{
        "d_id": "NatT_5507_FT",
        "predicate": "记得",
        "text": "侯博还记得她叫白冰，是那个大街上对他吐露过内心恋情的女生。",
        "hypothesis": "她确实叫白冰。",
        "option": {
            "T": "真",
            "F": "假",
            "U": "不能确定",
            "R": "模型拒绝回答"
        }
    }
```

### 2.3.3 人造语料样例集数据样例：

```json
{
        "d_id": "ArtS_001",
        "type": "非叙实",
        "predicate": "估计",
        "text": "小张估计小李没生病。",
        "hypothesis": "小李生病了。",
        "option": {
            "T": "真",
            "F": "假",
            "U": "不能确定",
            "R": "模型拒绝回答"
        },
        "answer": "U"
    }
```

### 2.3.4 真实语料样例集数据样例：

```json
{
        "d_id": "NatS_001",
        "predicate": "抱怨",
        "text": "世纪交替之际，联合国各会员国接受会费分摊比额新方案，体现了各国决心加强联合国作用的政治意愿。美国曾长期抱怨自己分摊的比例过大，要求削减会费。现在，美国的这一要求已在很大程度上得到满足。",
        "hypothesis": "美国分摊的比例确实过大。",
        "option": {
            "T": "真",
            "F": "假",
            "U": "不能确定",
            "R": "模型拒绝回答"
        },
        "answer": "T"
    }
```

## 2.4	任务描述

- 参赛队伍需要自行选定若干大型语言模型（型号与规模不限）；利用测试集数据编制提示词，并以API访问的方式逐条发送给被试模型；要求模型以"text"字段值为依据来判断"hypothesis"字段值的真值情况，记录模型的返回结果；最终将结果整理为JSON格式的数据文件。
- 真值情况包括三种：
  - 若根据"text"判定"hypothesis"为真，意味着根据背景句推出结论句的真值为“真”，此时请输出“T”；
  - 若根据"text"判定"hypothesis"为假，意味着根据背景句推出结论句的真值为“假”，此时请输出“F”；
  - 若根据"text"不能判定"hypothesis"的真假，意味着根据背景句推出结论句的真值为“不能确定”，此时请输出“U”。
- 若模型拒绝回答，请输出“R”。
- 若遇到其他问题，请邮件联系任务负责人。
- 参赛队伍使用的所有资源需要在最终提交的技术报告中给予详细说明。实验中的所有代码与结果请妥善保存，以备查用。

## 2.5	数据使用说明与提示

- 参赛队伍需要参考数据内容自行设计与大模型对话时的提示词，因此在数据中未设置"question"字段。
- 提示词格式不限，但在提示词中必须同时包含当前数据中"text"和"hypothesis"字段的内容。
- "type"和"predicate"字段在提示词中既可以出现，也可以不出现。请自行决定是否使用、如何使用"type"和"predicate"字段。
- 提示词的设计可以进行多样化尝试，如提供更多数量的shots、要求使用CoT、要求进行一致性投票、告知动词类型、告知动词的叙实性类型、变换提问句式等等。

<div id="daimashili"></div>

## 2.6	输出要求

- 结果文件须为JSON格式，其中每条数据只需包含"d_id"和"answer"两个字段即可。注意：微调赛道的数据id末尾以_FT结尾；不微调赛道的数据id末尾以_prompt结尾。务请看清赛道提交结果。
- 由于所有题目都是单选题，一条数据的"answer"处只允许填写一个值。
- 禁止对模型回答进行人工修正。
- 允许使用代码对模型回答进行统一提取，但设计代码时需要注意可复现性。同时，如果模型回答中出现真假判断前后不一致的情况，不可以只提取其中一种判断，而需要重新调整提示词。
- 无论微调还是不微调赛道，在上传的结果文件中必须包含评测集的全部数据（共2038条），否则在天池平台上提交结果时会显示报错。
- 结果文件中自然语料与人工语料的顺序安排对评测结果没有影响，只要自然语料的结果与人工语料的结果存放在同一个json文件中即可。

## 2.7	输出样例

- 不微调赛道

```json
{
        "d_id": "Nat_0001_prompt",
        "answer": "T"
    },
{
        "d_id": "Art_0001_prompt",
        "answer": "F"
    }
```

- 微调赛道

```json
{
        "d_id": "Nat_0001_FT",
        "answer": "T"
    },
{
        "d_id": "Art_0001_FT",
        "answer": "F"
    }
```

## 2.8	评测过程举例

下面我们以编号为"NatS_001"的真实语料样例集中的数据为例展示提示词设计赛道（不微调模型）的评测过程。

- 对于这条数据，我们可以将其改编为一段提示词（prompt）：（注意，此处提示词仅用于介绍举例，不作为模板使用。组织方在正式评测中仅提供数据集，参赛队伍需要充分利用数据信息，尽可能地优化提示词的设计）

```
仅根据text字段的内容，回答hypothesis字段的内容是否为真；只能用“真”“假”或“不能确定”来回答，不能回复其他内容。
"text": "世纪交替之际，联合国各会员国接受会费分摊比额新方案，体现了各国决心加强联合国作用的政治意愿。美国曾长期抱怨自己分摊的比例过大，要求削减会费。现在，美国的这一要求已在很大程度上得到满足。",
"hypothesis": "美国分摊的比例确实过大。"
```

- 选择某大模型（如GPT-o1），通过API访问的方式向其发送这段提示词，等待模型回复。
- 收到模型回答后，将其整理为JSON格式的数据（如下）：

```json
{
        "d_id": "NatS_001",
        "answer": "T"
    }
```
## 2.9	大模型微调设定

经讨论，本次评测将区分“微调模型”与“不微调模型（提示词设计）”两个测试方向。微调方向允许利用样例集数据对模型进行微调（新版样例集已上传，包含1000条数据）；不微调方向则不允许对模型进行任何修改，参赛队伍只能通过优化提示词设计来获取回答。两个测试方向将分开评奖，参赛队伍既可以同时选择参加两个方向的测试，也可以只选择参加其中一个方向的测试。无论选择哪个测试方向，整个测试过程均需在评测报告中作详细说明。


## 2.10 测试基线 （Baseline）

| 微调模型：Qwen2-7B-Instruct | 微调前 | 微调后 |
| ----------------- | ------- | ------- |
| 人造语料集正确率 | 53.74% | 94.66% |
| 自然语料集正确率 | 69.86% | 88.93% |

<div id="weitiao"></div>

## 2.11 微调方式参考

基线测试中使用的微调方式已公开，请参考：https://github.com/UM-FAH-Yuan/Factivity-LLM 。

此外，为了方便有需求的队伍进行微调，我们录制了一个访谈式的视频，以简要介绍一种利用Llama Factory来进行lora微调的方法。如需参考，请见https://meeting.tencent.com/crm/KPGoe6dYa8 （密码：0328）。


备注：由于样例集数据量限制，在训练模型时需要自行从样例集中拆分出一部分数据用作验证，因而可能无法完全复现基线结果。


# 3 评价标准 

本次评测采用总正确率作为评价指标:

<div align=center>
<img src="metric.jpg" width="400px">
</div>

其中，**total\_acc** 为总正确率，**correct\_art** 为人造语料集中模型回答正确的数据量，**correct\_nat** 为真实语料集中模型回答正确的数据量，**total\_art** 为人造语料集中的数据总量，**total\_nat** 为真实语料集中的数据总量。

备注：“微调模型”方向将与“不微调模型”方向分开评比。

# 4 技术报告要求 

参与评测必须提交技术报告，不提交技术报告的队伍成绩将不会获得认可。报告要求如下：

1.	报告可由中文或英文撰写。
2.	报告统一使用CCL 2025的论文模板。
3.	报告正文不得超过4页，参考文献页数不限。
4.	报告应至少包含以下四个部分：模型介绍、评测结果、结果分析与讨论和参考文献。
   
如打算向会议投稿，请参考下面的论文格式：

会议投稿需统一使用LaTeX模板。提交的论文最多包含 6 页正文，参考文献页数不限。由于本次会议采用双盲审稿，作者姓名和单位不能出现在投稿的论文中。因此，作者的自引不可采用“我们提出”的方式，而是用“作者名字提出…”。不符合这些要求的论文将不经过完整的审稿流程而直接被拒稿。论文模板下载链接：[http://cips-cl.org/static/CCL2024/downloads/ccl2023_template.zip](http://cips-cl.org/static/CCL2023/downloads/ccl2023_template.zip)（模板可能更新，请保持关注）。

# 5 组织方团队

- 任务组织者：袁毓林（澳门大学教授，yulinyuan@um.edu.mo）、李斌（南京师范大学教授）

- 任务负责人 & 联系人：丛冠良（澳门大学博士生，guanliang.cong@connect.um.edu.mo）

- 团队成员：吴俊潮（澳门大学博士生）、汪月童（澳门大学本科生）、陈可盈（澳门大学硕士生）、寻天琦（澳门大学博士生）、陈阳（澳门大学博士生）、杨梓泓（澳门大学硕士生）
  

# 6 任务奖项 

本届评测将为“微调”与“不微调”两个测试方向分别设置一、二、三等奖，奖项按总正确率从高到低颁发（奖金待定）。其中，每个方向一等奖0-1名，二等奖0-2名，三等奖0-3名。荣誉证书将由中国中文信息学会颁发。

# 7 任务网址 

https://github.com/UM-FAH-Yuan/FIE2025

# 8 参考文献（持续更新）

[1]陈振宇 & 姜毅宁.(2018).事实性与叙实性——通向直陈世界的晦暗与透明. 语言研究集刊(01),15-37+372-373. doi:CNKI:SUN:YJJK.0.2018-01-002.

[2]袁毓林.(2014).隐性否定动词的叙实性和极项允准功能. 语言科学(06),575-586. doi:CNKI:SUN:YYKE.0.2014-06-002.

[3]袁毓林.(2020).“忘记”类动词的叙实性漂移及其概念结构基础. 中国语文(05),515-526+638. doi:CNKI:SUN:YWZG.0.2020-05-001.

[4]袁毓林.(2020).叙实性和事实性：语言推理的两种导航机制. 语文研究(01),1-9. doi:CNKI:SUN:YWYJ.0.2020-01-001.

[5]袁毓林.(2020).“记得”的叙实性漂移及其概念结构基础. 语言教学与研究(01),36-47. doi:CNKI:SUN:YYJX.0.2020-01-007.

[6]袁毓林.(2021).从语言的“多声性”看“假装”句的解读歧异. 语言战略研究(05),77-90. doi:10.19689/j.cnki.cn10-1361/h.20210506.

[7]张帆.(2024).“假装”类动词宾语的类型及其真值判定理据. 中国语言学报(00),157-170. doi:CNKI:SUN:XBYT.0.2024-00-012.

[8]李新良.(2018).“感觉”类动词的叙实性及其漂移问题研究. 语言教学与研究(05),65-75. doi:CNKI:SUN:YYJX.0.2018-05-007.

[9]李新良.(2020). 现代汉语动词的叙实性研究. 北京: 北京大学出版社.

[10]李新良 & 袁毓林.(2016).反叙实动词宾语真假的语法条件及其概念动因. 当代语言学(02),194-215. doi:CNKI:SUN:DDYX.0.2016-02-004.

[11]李新良 & 袁毓林.(2017).“知道”的叙实性及其置信度变异的语法环境. 中国语文(01),42-52+127. doi:CNKI:SUN:YWZG.0.2017-01-003.

[12]李新良、袁毓林等.(2023). 叙实性与事实性理论及其运用. 北京: 外语教学与研究出版社.

[13]Kiparsky & Kiparsky. (1970). Fact. In M. Bierwisch & K. Heidolph (eds.). Progress in Linguistics. The Hague: Mouton. 143-147.


